# 网段规划

| service | cidr | port  |
| --- | --- |-------|
| mongo | 172.10.0.0/16 | 27017 |
| zookeeper | 172.11.0.0/16 | 2181  |
| app | 172.12.0.0/16 | 5000  |

docker不同网络连接到不同的网桥上，默认相互之间是隔离的，这里为了让app能够访问mongo和zookeeper，我们需要打通app到mongo和zookeeper监听的端口。

```
Chain DOCKER-ISOLATION-STAGE-1 (1 references)
 pkts bytes target     prot opt in     out     source               destination         
 1695  102K DOCKER-ISOLATION-STAGE-2  all  --  br-9cf6abbde8ee !br-9cf6abbde8ee  0.0.0.0/0            0.0.0.0/0           
    0     0 DOCKER-ISOLATION-STAGE-2  all  --  br-b7c620e1e025 !br-b7c620e1e025  0.0.0.0/0            0.0.0.0/0           
    0     0 DOCKER-ISOLATION-STAGE-2  all  --  br-e3808dbdac76 !br-e3808dbdac76  0.0.0.0/0            0.0.0.0/0           
14060  819K DOCKER-ISOLATION-STAGE-2  all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0           
30335   44M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain DOCKER-ISOLATION-STAGE-2 (4 references)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 DROP       all  --  *      br-9cf6abbde8ee  0.0.0.0/0            0.0.0.0/0           
  691 41460 DROP       all  --  *      br-b7c620e1e025  0.0.0.0/0            0.0.0.0/0           
 1004 60240 DROP       all  --  *      br-e3808dbdac76  0.0.0.0/0            0.0.0.0/0           
    0     0 DROP       all  --  *      docker0  0.0.0.0/0            0.0.0.0/0           
14060  819K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0       

NETWORK ID     NAME            DRIVER    SCOPE
e38619998c5b   bridge          bridge    local
2bc28b3303b8   host            host      local
e3808dbdac76   net_mongo       bridge    local
9cf6abbde8ee   net_app         bridge    local
5a027306fef6   none            null      local
b7c620e1e025   net_zookeeper   bridge    local
```

执行如下命令放通网络：

```shell
iptables -I DOCKER-USER -i br-9cf6abbde8ee -o br-e3808dbdac76 -p tcp --dport 27017 -j ACCEPT
iptables -I DOCKER-USER -o br-9cf6abbde8ee -i br-e3808dbdac76 -p tcp --sport 27017 -j ACCEPT
iptables -I DOCKER-USER -i br-9cf6abbde8ee -o br-b7c620e1e025 -p tcp --dport 2181 -j ACCEPT
iptables -I DOCKER-USER -o br-9cf6abbde8ee -i br-b7c620e1e025 -p tcp --sport 2181 -j ACCEPT
```

# 后端部署

为了实现高可用，我们先部署三个后端服务，此处我们通过三个docker容器模拟，对外分别暴露5001,5002,5003端口。

```shell
PROJECT_BASE=$(pwd)
docker network create --subnet=172.12.0.0/16 net_app
docker run -d --name app1 --network net_app --ip 172.12.0.100 -v $PROJECT_BASE/target:/root/app:z -p 5001:5000 -t openjdk:17-jdk-alpine /bin/sh -c 'java -jar /root/app/java-tutorial-*.jar'
docker run -d --name app2 --network net_app --ip 172.12.0.101 -v $PROJECT_BASE/target:/root/app:z -p 5002:5000 -t openjdk:17-jdk-alpine /bin/sh -c 'java -jar /root/app/java-tutorial-*.jar'
docker run -d --name app3 --network net_app --ip 172.12.0.102 -v $PROJECT_BASE/target:/root/app:z -p 5003:5000 -t openjdk:17-jdk-alpine /bin/sh -c 'java -jar /root/app/java-tutorial-*.jar'
```

# 后端负载均衡

这里我们使用nginx实现三个后端容器的负载均衡

```shell
PROJECT_BASE=$(pwd)
docker pull nginx
docker run -d --network net_app --ip 172.12.0.201 -p 4999:5000 -v $PROJECT_BASE/nginx/nginx1.conf:/etc/nginx/nginx1.conf --name shinerio-nginx nginx:latest
```

此时我们形成如下的架构，通过访问172.12.0.201:5000即可实现负载均衡访问后端的三个容器

![](https://shinerio.oss-cn-beijing.aliyuncs.com/blog_images/uncategory20220209222121.png)

# 双机热备

使用nginx作为负载均衡器后，nginx存在单点故障风险，因此引入keepalived实现双机热主备，形成如下架构

![](https://shinerio.oss-cn-beijing.aliyuncs.com/blog_images/uncategory20220209223438.png)

# **安装keepalived**

由于keepalived需要添加新的vip在eth0网口，因此容器启动的时候需要添加`--privileged`参数，否则docker其实只具有一个普通的用户权限，keepalived无法正常启动。

以--privileged方式重新创建2个nginx

```shell
PROJECT_BASE=$(pwd)
docker run -d --network net_app --ip 172.12.0.201 -p 4999:5000 -v $PROJECT_BASE/nginx/nginx1.conf:/etc/nginx/nginx.conf -v $PROJECT_BASE/nginx/keepalived-MASTER.conf:/etc/keepalived/keepalived.conf -v $PROJECT_BASE/nginx/nginx_check.sh:/etc/keepalived/nginx_check.sh --name shinerio-nginx1 --privileged nginx:latest
docker run -d --network net_app --ip 172.12.0.202 -p 4998:5000 -v $PROJECT_BASE/nginx/nginx2.conf:/etc/nginx/nginx.conf -v $PROJECT_BASE/nginx/keepalived-BACKUP.conf:/etc/keepalived/keepalived.conf -v $PROJECT_BASE/nginx/nginx_check.sh:/etc/keepalived/nginx_check.sh --name shinerio-nginx2 --privileged nginx:latest
```

在nginx容器中安装keepalived

```shell
apt update
apt install procps psmisc keepalived -y
service keepalived start
```

可以看到nginx1容器多了个172.12.0.200的vip，nginx1故障后，vip会自动切换到nginx2容器中。

![](https://shinerio.oss-cn-beijing.aliyuncs.com/blog_images/20220213184811.png)

此时通过172.12.0.200:5000,172.12.0.201:5000,172.12.0.202:5000均可正常访问服务。

> keepalived启动失败，可以通过查看/var/log/message日志分析原因。Deepin系统未安装rsyslog，可以通过apt install rsyslog安装

# 智能dns

# 参考链接

1. [Error: Unable to access jarfile](https://blog.csdn.net/wfq784967698/article/details/99412800)
2. https://help.fanruan.com/finereport/doc-view-2905.html
3. https://blog.51cto.com/h11345/1570786